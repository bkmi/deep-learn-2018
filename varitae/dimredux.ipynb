{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dimredux-challenge.npz\", \"rb\") as f:\n",
    "    X = np.load(f)\n",
    "    data = X['data']\n",
    "    dtraj = X['dtraj']\n",
    "    \n",
    "print(\"got data with shape {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=data[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    x_meanfree = (x - np.mean(x, axis=0))\n",
    "    C = (1 / (x.shape[0]-1)) * x_meanfree.T @ x_meanfree\n",
    "\n",
    "    eigenvalues, W  = np.linalg.eigh(C)\n",
    "    sig_inv = np.diag(1./np.sqrt(np.abs(eigenvalues) + 1e-6))\n",
    "    C_sqrt_inv = W @ sig_inv @ W.T\n",
    "\n",
    "    whitened = x_meanfree @ C_sqrt_inv\n",
    "    return whitened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111, projection='3d')\n",
    "ax.scatter(*(whiten(data).T), c=whiten(data)[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler divergence $D_{\\text{KL}}(P \\| Q) := -\\sum_i P(i)\\log \\frac{Q(i)}{P(i)} = \\sum_i P(i)\\log \\frac{P(i)}{Q(i)}$.\n",
    "\n",
    "Latent loss $D_{\\text{KL}}(P \\| \\mathcal{N}(0,1) ) = D_\\text{KL}(\\mathcal{N}(\\mu_\\text{latent}, \\sigma_\\text{latent}) \\| \\mathcal{N}(0,1))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TVAE(object):\n",
    "    \n",
    "    def __init__(self, enc_units, dec_units, ndim=3, latent_dim=2, activation=tf.nn.leaky_relu):\n",
    "        self.tf_is_traing_pl = tf.placeholder_with_default(True, shape=())\n",
    "        self.features = tf.placeholder(tf.float32, shape=[None, ndim])\n",
    "        self.timelagged_features = tf.placeholder(tf.float32, shape=[None, ndim])\n",
    "        self.latent_dim = latent_dim\n",
    "        x = tf.layers.dense(self.features, units=enc_units[0], activation=activation)\n",
    "        x = tf.layers.dropout(x, rate=.5, training=self.tf_is_traing_pl)\n",
    "        for units in enc_units[1:]:\n",
    "            x = tf.layers.dense(x, units=units, activation=activation)\n",
    "            x = tf.layers.dropout(x, rate=.5, training=self.tf_is_traing_pl)\n",
    "        latent_mean = tf.layers.dense(x, units=latent_dim, activation=None)\n",
    "        latent_log_std_sq = tf.layers.dense(x, units=latent_dim, activation=None)\n",
    "        \n",
    "        samples = tf.random_normal((tf.shape(self.features)[0], latent_dim), mean=0., stddev=1., dtype=tf.float32)\n",
    "        self.z = latent_mean + tf.sqrt(tf.exp(latent_log_std_sq)) * samples\n",
    "        \n",
    "        x = tf.layers.dense(self.z, units=dec_units[0], activation=activation)\n",
    "        x = tf.layers.dropout(x, rate=.5, training=self.tf_is_traing_pl)\n",
    "        for units in dec_units[1:]:\n",
    "            x = tf.layers.dense(x, units=units, activation=activation)\n",
    "            x = tf.layers.dropout(x, rate=.5, training=self.tf_is_traing_pl)\n",
    "        self.decoded = tf.layers.dense(x, units=ndim, activation=None)\n",
    "        \n",
    "        reconstruction_loss = .5*tf.reduce_sum(tf.pow(self.decoded - self.timelagged_features, 2.0))\n",
    "        self.reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "        latent_loss = -.5 * tf.reduce_sum(1 + latent_log_std_sq - tf.sent_mean = tf.layers.dense(x, units=latent_dim, activation=None)\n",
    "        latent_logquare(latent_mean) \n",
    "                                          - tf.exp(latent_log_std_sq), axis=1)\n",
    "        self.latent_loss = tf.reduce_mean(latent_loss)\n",
    "        self.loss = tf.reduce_mean(latent_loss + reconstruction_loss)\n",
    "        self.train_operation = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(self.loss)\n",
    "        \n",
    "    def train(self, session, chunk, chunk_lagged):\n",
    "        _, loss, reconstruction_loss, latent_loss = session.run(\n",
    "            (self.train_operation, self.loss, self.reconstruction_loss, self.latent_loss),\n",
    "            feed_dict={self.features: chunk, self.timelagged_features: chunk_lagged, self.tf_is_traing_pl: True}\n",
    "        )\n",
    "        return loss, reconstruction_loss, latent_loss\n",
    "    \n",
    "    def predict(self, session, batch):\n",
    "        return session.run(self.decoded, \n",
    "                           feed_dict={self.features: batch, \n",
    "                                      self.tf_is_traing_pl: False})\n",
    "    \n",
    "    def decode(self, session, z):\n",
    "        return session.run(self.decoded, \n",
    "                           feed_dict={self.z: z, self.tf_is_traing_pl: False})\n",
    "    \n",
    "    def encode(self, session, batch):\n",
    "        return session.run(self.z, feed_dict={self.features: batch, self.tf_is_traing_pl: False})\n",
    "    \n",
    "    def generate(self, session, n=1, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = session.run(tf.random_normal([n, self.latent_dim]))\n",
    "        return session.run(self.decoded, \n",
    "                           feed_dict={self.z: hidden})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "n_epochs = 500\n",
    "lag=1\n",
    "\n",
    "X = whiten(data[:-lag])\n",
    "Y = whiten(data[lag:])\n",
    "\n",
    "all_losses = []\n",
    "all_reconstruction_losses = []\n",
    "all_latent_losses = []\n",
    "\n",
    "tvae = TVAE([300,150], [150,300], ndim=3, latent_dim=1, activation=tf.nn.leaky_relu)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        reconstruction_losses = []\n",
    "        latent_losses = []\n",
    "        for ix in range(len(X) // batch_size):\n",
    "            batch = X[ix*batch_size : min((ix+1)*batch_size, len(X))]\n",
    "            batch_lagged = Y[ix*batch_size : min((ix+1)*batch_size, len(Y))]\n",
    "            if len(batch) == len(batch_lagged):\n",
    "                out = tvae.train(sess, batch, batch_lagged)\n",
    "                losses.append(out[0])\n",
    "                reconstruction_losses.append(out[1])\n",
    "                latent_losses.append(out[2])\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"epoch {}: loss {}\".format(epoch, np.mean(losses)))\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/tvae/model-{}.ckpt\".format(epoch+1))\n",
    "            print(\"Model saved in path: {}\".format(save_path))\n",
    "        all_losses.append(np.mean(losses))\n",
    "        all_reconstruction_losses.append(np.mean(reconstruction_losses))\n",
    "        all_latent_losses.append(np.mean(latent_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(all_losses, 'x', label='total')\n",
    "plt.loglog(all_latent_losses, label='latent')\n",
    "plt.loglog(all_reconstruction_losses, label='reconstruction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"/tmp/tvae/model-500.ckpt\")\n",
    "    latent = tvae.encode(sess, whiten(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = KMeans(n_clusters=4).fit(latent).cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1,figsize=(15, 5))\n",
    "ax.plot(latent[:1000])\n",
    "ax.hlines(cc, xmin=0, xmax=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = np.abs(latent[:, 0, None] - cc.squeeze()[None, :]).argmin(axis=1) #np.array([-1.5, -.5, .5, 1.5])\n",
    "np.max([np.sum(np.array(p)[dtraj] == guess) for p in itertools.permutations([1,2,3,0])]) * 100. / len(dtraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TAE(object):\n",
    "    \n",
    "    def __init__(self, enc_units, dec_units, ndim=3, latent_dim=2, activation=tf.nn.leaky_relu, dropout=.3):\n",
    "        self.tf_is_traing_pl = tf.placeholder_with_default(True, shape=())\n",
    "        self.features = tf.placeholder(tf.float32, shape=[None, ndim])\n",
    "        self.timelagged_features = tf.placeholder(tf.float32, shape=[None, ndim])\n",
    "        x = tf.layers.batch_normalization(self.features)\n",
    "        x = tf.layers.dense(x, units=enc_units[0], activation=activation)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=self.tf_is_traing_pl)\n",
    "        for units in enc_units[1:]:\n",
    "            x = tf.layers.dense(x, units=units, activation=activation)\n",
    "            x = tf.layers.dropout(x, rate=dropout, training=self.tf_is_traing_pl)\n",
    "        \n",
    "        self.z = tf.layers.dense(x, latent_dim, activation=None)\n",
    "        \n",
    "        x = tf.layers.dense(self.z, units=dec_units[0], activation=activation)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=self.tf_is_traing_pl)\n",
    "        for units in dec_units[1:]:\n",
    "            x = tf.layers.dense(x, units=units, activation=activation)\n",
    "            x = tf.layers.dropout(x, rate=dropout, training=self.tf_is_traing_pl)\n",
    "        self.decoded = tf.layers.dense(x, units=ndim, activation=None)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(self.timelagged_features, self.decoded)\n",
    "        self.train_operation = tf.train.AdamOptimizer(learning_rate=5e-5).minimize(self.loss)\n",
    "        \n",
    "    def train(self, session, X, Y):\n",
    "        self.training = True\n",
    "        _, loss, = session.run(\n",
    "            (self.train_operation, self.loss),\n",
    "            feed_dict={self.features: X, self.timelagged_features: Y, self.tf_is_traing_pl: True}\n",
    "        )\n",
    "        self.training = False\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, session, batch):\n",
    "        return session.run(self.decoded, feed_dict={self.features: batch, self.tf_is_traing_pl: False})\n",
    "    \n",
    "    def decode(self, session, z):\n",
    "        return session.run(self.decoded, feed_dict={self.z: z, self.tf_is_traing_pl: False})\n",
    "    \n",
    "    def encode(self, session, batch):\n",
    "        return session.run(self.z, feed_dict={self.features: batch, self.tf_is_traing_pl: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "n_epochs = 2000\n",
    "lag=1\n",
    "\n",
    "all_losses = []\n",
    "all_reconstruction_losses = []\n",
    "all_latent_losses = []\n",
    "\n",
    "X = whiten(data[:-lag])\n",
    "Y = whiten(data[lag:])\n",
    "\n",
    "tae = TAE([200,100], [100,200], ndim=3, latent_dim=1, activation=tf.nn.leaky_relu, dropout=.5)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        for ix in range(len(data) // batch_size):\n",
    "            batch = X[ix*batch_size : min((ix+1)*batch_size, len(X))]\n",
    "            batch_lagged = Y[ix*batch_size : min((ix+1)*batch_size, len(Y))]\n",
    "            if len(batch) == len(batch_lagged):\n",
    "                out = tae.train(sess, batch, batch_lagged)\n",
    "                losses.append(out)\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"epoch {}: loss {}\".format(epoch, np.mean(losses)))\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/tae/model-{}.ckpt\".format(epoch+1))\n",
    "            print(\"Model saved in path: {}\".format(save_path))\n",
    "        all_losses.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"/tmp/tae/model-2000.ckpt\")\n",
    "    latent = tae.encode(sess, whiten(data))  # whiten(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = KMeans(n_clusters=4).fit(latent).cluster_centers_\n",
    "# cc = [-0.05, .07, .2, -.2]\n",
    "f, ax = plt.subplots(1,1, figsize=(15, 5))\n",
    "ax.plot(latent[:1000])\n",
    "ax.hlines(cc, xmin=0, xmax=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = np.abs(latent[:, 0, None] - np.array(cc).squeeze()[None, :]).argmin(axis=1)\n",
    "perms = [np.sum(np.array(p)[dtraj] == guess) for p in itertools.permutations([1,2,3,0])]\n",
    "percent = np.max(perms) * 100. / len(dtraj)\n",
    "print(\"{:.4f}%\".format(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
